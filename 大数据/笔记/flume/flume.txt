一、什么是Flume：
	概念
		flume是分布式的，可靠的，用于从不同的来源有效收集 聚集 和 移动 大量的日志数据用以集中式的数据存储的系统。

		是apache的一个顶级项目

	系统需求：
		jdk1.6以上，推荐java1.7

	数据流模型：
		一个Flume Event(Flume 事件)被定义为一个具有有效荷载的字节数据流和可选的字符串属性集。
		一个Flume Agent(Flume 代理) 是一个进程承载从外部源事件流到下一个目的地的过程。包含source channel 和 sink
		一个Flume Source(Flume 数据源)消耗外部传递给他的事件，外部源将数据按照Flume Source 能识别的格式将Flume 事件发送给Flume Source
		一个Flume Sink(Flume 汇聚点)
		一个Flume channle 是一个被动的存储，用来保持事件，知道由一个Flume Sink消耗。
	复杂流动
		Flume允许用户进行多跳流转到最终目的地，也允许一到多、多到一的流动和故障转移、失败处理。
	可靠性
		事务型的数据传递，保证数据的可靠性。
	可恢复
		通道可以以内存或文件的方式实现，内存更快，但是不可恢复，文件比较慢但提供了可恢复性。

二、设置
	配置文件
		Flume使用properties文件作为配置文件。一个配置文件中可以配置一个或多个Flume Agent。	
		这个配置文件声明Agent中使用到的Source、Sink、Channel,并说明他们是如何组合在一起，形成数据流的。
	配置单个部件
		每个部件（Source、Sink、Channel）都有一个名称、类型，并且通过属性指定特定的类型和实例。
	连接拼凑
		通过指定channle和source、sink之间的关系来拼接各个部件。
	启动Agent
		通过bin目录下的flume-ng命令启动一个agent，在启动时需要指定agent名称，config目录，配置文件。
		$ bin/flume-ng agent -n $agent_name -c conf -f conf/flume-conf.properties.template
三、简单例子
	我们举一个配置文件的例子，描述了一个单节点水槽的部署，允许用户产生事件和随后将它们记录到控制台。

	--
	＃example.conf：单节点Flume配置

	＃命名Agent ai的组件
	a1.sources  =  R1 
	a1.sinks  =  K1 
	a1.channels  =  C1

	＃描述/配置Source
	a1.sources.r1.type  =  netcat
	a1.sources.r1.bind  =  localhost
	a1.sources.r1.port  =  44444

	＃描述Sink
	a1.sinks.k1.type  =  logger

	＃描述内存Channel
	a1.channels.c1.type  =  memory
	a1.channels.c1.capacity  =  1000 
	a1.channels.c1.transactionCapacity  =  100

	＃为Channle绑定Source和Sink
	a1.sources.r1.channels  =  C1 
	a1.sinks.k1.channel  =  C1
	--
	该配置定义了一个名为a1的单一Agent，a1中定义了一个监听44444端口的数据源、一个内存中的Channel和一个Sink，记录事件数据到控制台。
	一个配置文件中可以定义多个Agent.
	在启动时，需要指定启动的是哪个名称的Agent。

	在通过命令行启动Flume执行Agent a1:
	$ bin/flume-ng agent --conf conf --conf-file example.conf --name a1 -Dflume.root.logger=INFO,console

	**注意，在真实开发中，通产给我们会通过一个 --conf-dir 参数指定一个目录，其中应该包含一个 flume-env.sh和一个log4j配置文件。在这个例子中我们通过一个JAVA选项强制控制Flume将日志打印到控制台，而没有指定该选项。


	通过telnet进行测试：
	--
	$ telnet localhost 44444
	Trying 127.0.0.1...
	Connected to localhost.localdomain (127.0.0.1).
	Escape character is '^]'.
	Hello world! <ENTER>
	OK
	--

	此时Flume向控制台输出：
	--
	12/06/19 15:32:19 INFO source.NetcatSource: Source starting
	12/06/19 15:32:19 INFO source.NetcatSource: Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/127.0.0.1:44444]
	12/06/19 15:32:34 INFO sink.LoggerSink: Event: { headers:{} body: 48 65 6C 6C 6F 20 77 6F 72 6C 64 21 0D          Hello world!. }
	--

	恭喜你，成功配置和运行了一个Flume agent! 随后的章节将介绍更为详细的配置。

四、通过zookeeper管理
	Flume可以通过zookeeper管理。这是一个试验性质的功能。
	配置文件可以上传到zookeeper中通过可配置的前缀进行管理。
	配置文件存储在zookeeper的数据节点中。

	下面是一个例子：
	--
	- /flume
	 |- /a1 [Agent config file]
	 |- /a2 [Agent config file]
	--

	上传好配置文件后，通过以下启动参数使用zookeeper中的配置：
	$ bin/flume-ng agent -conf conf -z zkhost:2181,zkhost1:2181 -p /flume –name a1 -Dflume.root.logger=INFO,console
	参数说明：
		名称	默认值	说明
		z	–		zookeeper连接字符串
		p	/flume	Flume的Agent配置文件在zookeeper中的基目录

五、第三方插件
	Flume以完全插件的方式进行架构。
	虽然Flume附带了许多现成的Source、Channle、Sink、序列化工具等，但它同样支持第三方插件。
	可以通过在flume-env.sh文件中通过FLUME_CLASSPATH变量增加jar包路径，但更好的方法时，将按照特定格式封装的插件直接放置到plugins.d目录下，Flume将会自动管理该目录下的插件。这使得插件管理更为简单，也更容易调试和故障排除，特别是库依赖关系的管理。
	plugins.d目录通常在$FLUME_HOME/plugins.d。

	插件格式:
		每个插件最多可以有三个子目录：
			lib - 插件jar包所在目录
			libext - 插件依赖的jar所在的目录
			native - 任何所需本地库，如.so文件


六、数据的摄取
	Flume支持多种方式获取来自外部源的数据

	RPC
		在Flume中包含的Avro client可以将指定文件发送给Flume Avro Souce -- 通过RPC机制
		$ bin/flume-ng avro-client -H localhost -p 41414 -F /usr/logs/log.10

	Executing commands
		There’s an exec source that executes a given command and consumes the output. A single ‘line’ of output ie. text followed by carriage return (‘\r’) or line feed (‘\n’) or both together.
		Note Flume does not support tail as a source. One can wrap the tail command in an exec source to stream the file.

	网络流
		Flume支持如下流行的日志流机制：
			Avro
			Thrift
			Syslog
			Netcat
			
	设置多代理流程
		为了使数据流流过多个代理，需要当前sink指向下一个agent的source（通过ip/主机名和port）
	合并
		一个非常常见的场景是，在日志收集系统中，多个收集数据客户端将数据发送到少数存储系统中。
		这可以通过配置多个第一层的sink指向第二层的源来合并接收事件。第二层再通过一个单一通道将数据写入最终存储位置上。

	复流
		Flume支持多路复用事件流将数据流至一个或多个目的地。这需要一个多路复用器，将数据流复制或选择到多个Channel来实现。
		
七、配置
	Flume通过Properties文件进行配置。

	定义流程
		要定义单一Agent，需要通过一个Channel来连接Source和sink。你需要列出Source，Sink和Channle，然后将Source和Sink指定到一个Channel上。一个Source可以指向多个Channle，但是一个Sinke只能指向到一个Channel.

		--
		# list the sources, sinks and channels for the agent
		<Agent>.sources = <Source>
		<Agent>.sinks = <Sink>
		<Agent>.channels = <Channel1> <Channel2>

		# set channel for source
		<Agent>.sources.<Source>.channels = <Channel1> <Channel2> ...

		# set channel for sink
		<Agent>.sinks.<Sink>.channel = <Channel1>
		--
	配置单个部件
		--
		# properties for sources
		<Agent>.sources.<Source>.<someProperty> = <someValue>

		# properties for channels
		<Agent>.channel.<Channel>.<someProperty> = <someValue>

		# properties for sinks
		<Agent>.sources.<Sink>.<someProperty> = <someValue>
		--		

		每个Flume中的部件都需要指定type属性来使Flume知道这是一个什么样的组件。
		每个Source、skin、channle都有自己的一套属性来实现预定功能。

	一个Agent中配置多个Flume流
		一个Agent可以包含若干个单独的流，你可以列出多个Source Skin 和 Channel，然后将他们链接形成多个流。

		--
		＃列表中源，汇和渠道代理
		agent_foo.sources  =  的Avro-APPSRV-源1的exec-尾源2 
		agent_foo.sinks  =  HDFS-Cluster1中，信宿的Avro转发，信宿
		agent_foo.channels  =  MEM-通道-1文件-通道-2-

		＃流＃1 configuration 
		agent_foo.sources.avro-AppSrv-source1.channels  =  mem-channel-1 
		agent_foo.sinks.hdfs-Cluster1-sink1.channel  =  mem-channel-1

		＃流＃2 configuration 
		agent_foo.sources.exec-tail-source2.channels  =  file-channel-2 
		agent_foo.sinks.avro-forward-sink2.channel  =  file-channel-2
		--

	多层Agent流
		要配置一个多层的流动，需要从第一个sink 指向 下一个sink通过Avro/thrift。
		这将导致第一个Agent转发事件给第二个Agent。

		--
		Weblog agent config:

		# list sources, sinks and channels in the agent
		agent_foo.sources = avro-AppSrv-source
		agent_foo.sinks = avro-forward-sink
		agent_foo.channels = file-channel

		# define the flow
		agent_foo.sources.avro-AppSrv-source.channels = file-channel
		agent_foo.sinks.avro-forward-sink.channel = file-channel

		# avro sink properties
		agent_foo.sources.avro-forward-sink.type = avro
		agent_foo.sources.avro-forward-sink.hostname = 10.1.1.100
		agent_foo.sources.avro-forward-sink.port = 10000

		# configure other pieces
		#...


		HDFS agent config:

		# list sources, sinks and channels in the agent
		agent_foo.sources = avro-collection-source
		agent_foo.sinks = hdfs-sink
		agent_foo.channels = mem-channel

		# define the flow
		agent_foo.sources.avro-collection-source.channels = mem-channel
		agent_foo.sinks.hdfs-sink.channel = mem-channel

		# avro sink properties
		agent_foo.sources.avro-collection-source.type = avro
		agent_foo.sources.avro-collection-source.bind = 10.1.1.100
		agent_foo.sources.avro-collection-source.port = 10000

		# configure other pieces
		#...
		--
		在上面的例子中，我们从博客Agent到HDFS Agent间建立了一个链接。这将导致外部传入的事件最终存入HDFS中。


	扇出流（一到多）
		Flume支持一到多的扇出流。扇出流有复制和多路复用两种模式。
		在复制模式下，事件将发送到所有配置的通道中。
		在多路复用模式下，事件将只发送到有资格的通道中。这需要指定一个策略列表来进行。
		整个过程需要用到一个"选择器"，这个选择器可以被设置到赋值/多路复用状态下。
		如果是多路复用模式，还需要为选择器指定选择规则。
		如果没有指定，则默认是复制模式。
		--
		# List the sources, sinks and channels for the agent
		<Agent>.sources = <Source1>
		<Agent>.sinks = <Sink1> <Sink2>
		<Agent>.channels = <Channel1> <Channel2>

		# set list of channels for source (separated by space)
		<Agent>.sources.<Source1>.channels = <Channel1> <Channel2>

		# set channel for sinks
		<Agent>.sinks.<Sink1>.channel = <Channel1>
		<Agent>.sinks.<Sink2>.channel = <Channel2>

		<Agent>.sources.<Source1>.selector.type = replicating
		--
		多路复用模式还需要指定规则。选贼其检查每个事件头重的配置属性，如果指定属性和配置值相匹配，则该事件被发送到所有匹配的通道中，如果没有任何通道匹配，则发送到默认通道中。
		--
		# Mapping for multiplexing selector
		<Agent>.sources.<Source1>.selector.type = multiplexing
		<Agent>.sources.<Source1>.selector.header = <someHeader>
		<Agent>.sources.<Source1>.selector.mapping.<Value1> = <Channel1>
		<Agent>.sources.<Source1>.selector.mapping.<Value2> = <Channel1> <Channel2>
		<Agent>.sources.<Source1>.selector.mapping.<Value3> = <Channel2>
		#...

		<Agent>.sources.<Source1>.selector.default = <Channel2>
		--
		The mapping allows overlapping the channels for each value.

		案例：
		--
		# list the sources, sinks and channels in the agent
		agent_foo.sources = avro-AppSrv-source1
		agent_foo.sinks = hdfs-Cluster1-sink1 avro-forward-sink2
		agent_foo.channels = mem-channel-1 file-channel-2

		# set channels for source
		agent_foo.sources.avro-AppSrv-source1.channels = mem-channel-1 file-channel-2

		# set channel for sinks
		agent_foo.sinks.hdfs-Cluster1-sink1.channel = mem-channel-1
		agent_foo.sinks.avro-forward-sink2.channel = file-channel-2

		# channel selector configuration
		agent_foo.sources.avro-AppSrv-source1.selector.type = multiplexing
		agent_foo.sources.avro-AppSrv-source1.selector.header = State
		agent_foo.sources.avro-AppSrv-source1.selector.mapping.CA = mem-channel-1
		agent_foo.sources.avro-AppSrv-source1.selector.mapping.AZ = file-channel-2
		agent_foo.sources.avro-AppSrv-source1.selector.mapping.NY = mem-channel-1 file-channel-2
		agent_foo.sources.avro-AppSrv-source1.selector.default = mem-channel-1
		--
		在如上的例子中，选择器检查"State"头信息，如果改制是CA，则发送到mem-channel-1。如果该值是AZ，则发送到file-channel-2。如果是NY则同时发送到mem-channel-1 file-channel-2，其他情况发送到默认的 mem-channel-1中。

		也可以配置可选通道，改造如上例子最后几行：
		--
		# channel selector configuration
		agent_foo.sources.avro-AppSrv-source1.selector.type = multiplexing
		agent_foo.sources.avro-AppSrv-source1.selector.header = State
		agent_foo.sources.avro-AppSrv-source1.selector.mapping.CA = mem-channel-1
		agent_foo.sources.avro-AppSrv-source1.selector.mapping.AZ = file-channel-2
		agent_foo.sources.avro-AppSrv-source1.selector.mapping.NY = mem-channel-1 file-channel-2
		agent_foo.sources.avro-AppSrv-source1.selector.optional.CA = mem-channel-1 file-channel-2
		agent_foo.sources.avro-AppSrv-source1.selector.mapping.AZ = file-channel-2
		agent_foo.sources.avro-AppSrv-source1.selector.default = mem-channel-1
		--
		？？？？选择器将首先尝试写入事件到所有指定通道中，但这些指定通道中哪怕只有一个失败都会造成对所有通道的重新尝试写入。Once all required channels have consumed the events, then the selector will attempt to write to the optional channels.任何可选通道中的故障，事件将被简单的忽略而不是重试。
		如果一个通道被同时配置为必须和可选，则被认为是必须的，在他上的故障将导致事件在所有指定通道中重新执行。如上面例子中的CA
		如果一个事件不具有任何指定通道，则将会被写入默认通道和试图写入可选通道。
		如果一个事件指定了一个头信息，却没有对应通道，则仍然会写入到默认通道中。如果。。。
		The selector will attempt to write to the required channels first and will fail the transaction if even one of these channels fails to consume the events. The transaction is reattempted on all of the channels. Once all required channels have consumed the events, then the selector will attempt to write to the optional channels. A failure by any of the optional channels to consume the event is simply ignored and not retried.
		If there is an overlap between the optional channels and required channels for a specific header, the channel is considered to be required, and a failure in the channel will cause the entire set of required channels to be retried. For instance, in the above example, for the header “CA” mem-channel-1 is considered to be a required channel even though it is marked both as required and optional, and a failure to write to this channel will cause that event to be retried on all channels configured for the selector.
		Note that if a header does not have any required channels, then the event will be written to the default channels and will be attempted to be written to the optional channels for that header. Specifying optional channels will still cause the event to be written to the default channels, if no required channels are specified. If no channels are designated as default and there are no required, the selector will attempt to write the events to the optional channels. Any failures are simply ignored in that case.

八、Flume Source
	1.AVRO Source
		监听AVRO端口来接受来自外部AVRO客户端的事件流。当使用内置的Avro的水槽上的另一个（前一跳）水槽剂配合使用时，它可以创建分层集合拓扑。
		需要属性如下，加!的为必须属性：
		属性			默认值		说明
		!channels	–	 
		!type	–	类型名称，"AVRO"
		!bind	–	需要监听的主机名或IP
		!port	–	要监听的端口
		threads	–	工作线程最大线程数
		selector.type	 	 
		selector.*	 	 
		interceptors	–	空格分隔的拦截器列表
		interceptors.*	 	 
		compression-type	none	压缩类型，可以是“none”或“default”，这个值必须和AvroSource的压缩格式匹配
		ssl	false	是否启用ssl加密，如果启用还需要配置一个“keystore”和一个“keystore-password”.
		keystore	–	为SSL提供的 java密钥文件 所在路径
		keystore-password	–	为SSL提供的 java密钥文件 密码
		keystore-type	JKS	密钥库类型可以是 “JKS” 或 “PKCS12”.
		exclude-protocols	SSLv3	空格分隔开的列表，用来指定在SSL / TLS协议中排除。SSLv3将总是被排除除了所指定的协议。
		ipFilter	false	如果需要为netty开启ip过滤，将此项设置为true
		ipFilterRules	–	陪netty的ip过滤设置表达式规则

		ipFilterRules说明:
			使用如下语法：
				<allow 或 deny>:<ip 或 name>:<pattern>[,<allow 或 deny>:<ip 或 name>:<pattern> ...]
			例如:
				ipFilterRules=allow:ip:127.*,allow:name:localhost,deny:ip:*

	2.Thrift Source
		略

	3.Exec Source
		可以将命令产生的输出作为源

		参数说明，加叹号的为必要属性：
			!channels	–	 
			!type	–	类型名称，需要是"exec"
			!command	–	要执行的命令
			shell	–	A shell invocation used to run the command. e.g. /bin/sh -c. Required only for commands relying on shell features like wildcards, back ticks, pipes etc.
			restartThrottle	10000	毫秒为单位的时间，用来声明等待多久后尝试重试命令
			restart	false	如果cmd挂了，是否重启cmd
			logStdErr	false	无论是否是标准错误都该被记录
			batchSize	20	同时发送到通道中的最大行数
			batchTimeout	3000	如果缓冲区没有满，经过多长时间发送数据
			selector.type	复制还是多路复用
			selector.*	 	Depends on the selector.type value
			interceptors	–	空格分隔的拦截器列表
			interceptors.*	 	 

	4.JMS Sourc
		略

	5.Spooling Directory Source - 自动搜集目录源
		这个Source允许你将文件将要收集的数据放置到"自动搜集"目录中。这个Source将监视该目录，并将解析新文件的出现。事件处理逻辑是可插拔的，当一个文件被完全读入信道，它会被重命名或可选的直接删除。
		不同于Exec Source,这种Source是可靠的，不会错过数据--即使Flume被杀死或重启。为了保证这一点，"文件搜集"目录中的文件名必须是唯一不变的。如果违反了这一点，Flume将会试图以如下方式告知错误：
			~如果一个文件被放置到"文件搜集"目录后被修改过，Flume将会将错误打印到日志并停止处理。
			~如果一个文件名被重用，Flume将会将错误打印到日志并停止处理。
		为了避免如上的问题，这些文件可以用唯一标识符拼接（如时间戳）作为名称。

		参数说明，加叹号为必须属性：
			!channels	–	 
			!type	–	类型，需要指定为"spooldir"
			!spoolDir	–	读取文件的路径，即"搜集目录"
			fileSuffix	.COMPLETED	对处理完成的文件追加的后缀
			deletePolicy	never	处理完成后是否删除文件，需是"never"或"immediate"
			fileHeader	false	Whether to add a header storing the absolute path filename.
			fileHeaderKey	file	Header key to use when appending absolute path filename to event header.
			basenameHeader	false	Whether to add a header storing the basename of the file.
			basenameHeaderKey	basename	Header Key to use when appending basename of file to event header.
			ignorePattern	^$	正则表达式指定那些文件需要忽略
			trackerDir	.flumespool	Directory to store metadata related to processing of files. If this path is not an absolute path, then it is interpreted as relative to the spoolDir.
			consumeOrder	处理文件的策略，oldest, youngest 或 random。
			maxBackoff	4000	The maximum time (in millis) to wait between consecutive attempts to write to the channel(s) if the channel is full. The source will start at a low backoff and increase it exponentially each time the channel throws a ChannelException, upto the value specified by this parameter.
			batchSize	100	Granularity at which to batch transfer to the channel
			inputCharset	UTF-8	读取文件时使用的编码。
			decodeErrorPolicy	FAIL	当在输入文件中发现无法处理的字符编码时如何处理。FAIL：抛出一个异常而无法 ​​解析该文件。REPLACE：用“替换字符”字符，通常是Unicode的U + FFFD更换不可解析角色。 忽略：掉落的不可解析的字符序列。
			deserializer	LINE	声明用来将文件解析为事件的解析器。默认一行为一个事件。处理类必须实现EventDeserializer.Builder接口。
			deserializer.*	 	Varies per event deserializer.
			bufferMaxLines	–	(Obselete) This option is now ignored.
			bufferMaxLineLength	5000	(Deprecated) Maximum length of a line in the commit buffer. Use deserializer.maxLineLength instead.
			selector.type	replicating	replicating or multiplexing
			selector.*	 	Depends on the selector.type value
			interceptors	–	Space-separated list of interceptors
			interceptors.*	 	 

		事件转换器Event Deserializers--详见标题九：
			Line 行转换器，将一行转换为一个Event
			Avro 可以读取一个Avro格式数据的文件，一个Avro记录转换为一个Event。每个事件与头注释表明所使用的模式。Event的体是二进制Avro记录数据,不包括模式或其他容器文件元素。
			BlobDeserializer 可以读取一个二进制大文件。一个大文件转换为一个Event。如PDF或JPG文件。
			
	6.Twitter 1% firehose Source (experimental)
		略

	7.Kafka Source
		略

	8.NetCat Source
		一个NetCat Source用来监听一个指定端口，并将接收到的数据的每一行转换为一个事件。

		属性说明，加！的为必要属性：
			！channels	–	 
			！type	–	类型名称，需要被设置为"netcat"
			！bind	–	指定要绑定到的ip或主机名。
			！port	–	指定要绑定到的端口号
			max-line-length	512	单行最大字节数
			ack-every-event	true	对于收到的每一个Event是否响应"OK"
			selector.type	
			selector.*	 	
			interceptors	–	
			interceptors.*	 	

	9.Sequence Generator Source -- 序列发生器源
		一个简单的序列发生器，不断的产生事件，值是从0开始每次递增1。
		主要用来进行测试。
		参数说明， 加！为必须属性：
		!channels	–	 
		!type	–	类型名称，必须为"seq"
		selector.type	 	
		selector.*	
		interceptors	–	
		interceptors.*	 	 
		batchSize	1

	10.Syslog Source -- 注：在Unix类操作系统上，syslog广泛应用于系统日志
		略

	11.HTTP Source
		此Source接受HTTP的GET和POST请求作为Flume的事件。
		其中GET方式应该只用于试验。
		需要提供一个可插拔的"处理器"来将请求转换为事件对象，这个处理器必须实现HTTPSourceHandler接口
		这个处理器接受一个 HttpServletRequest对象，并返回一个Flume Envent对象集合。
		从一个HTTP请求中得到的事件将在一个事务中提交到通道中。thus allowing for increased efficiency on channels like the file channel。
		如果处理器抛出一个异常，Source将会返回一个400的HTTP状态码。
		如果通道已满，无法再将Event加入Channel，则Source返回503的HTTP状态码，表示暂时不可用。

		参数说明，加！的为必须属性:
			！type	 	类型，必须为"HTTP"
			！port	–	监听的端口
			bind	0.0.0.0	监听的主机名或ip
			handler	org.apache.flume.source.http.JSONHandler	处理器类，需要实现HTTPSourceHandler接口
			handler.*	–	处理器的配置参数
			selector.type	
			selector.*	 	
			interceptors	–	
			interceptors.*	 	 
			enableSSL	false	是否开启SSL,如果需要设置为true。注意，HTTP不支持SSLv3。
			excludeProtocols	SSLv3	空格分隔的要排除的SSL/TLS协议。SSLv3总是被排除的。
			keystore	 	密钥库文件所在位置。
			keystorePassword Keystore 密钥库密码

			常见handler：
				JSONHandler
					可以处理JSON格式的数据，并支持UTF-8 UTF-16 UTF-32字符集
					该handler接受Evnet数组，并根据请求头中指定的编码将其转换为Flume Event
					如果没有指定编码，默认编码为UTF-8.
					JSON格式如下：
					--
					[{
					  "headers" : {
					             "timestamp" : "434324343",
					             "host" : "random_host.example.com"
					             },
					  "body" : "random_body"
					  },
					  {
					  "headers" : {
					             "namenode" : "namenode.example.com",
					             "datanode" : "random_datanode.example.com"
					             },
					  "body" : "really_random_body"
					  }]
					--
				To set the charset, the request must have content type specified as application/json;charset=UTF-8 (replace UTF-8 with UTF-16 or UTF-32 as required).
				One way to create an event in the format expected by this handler is to use JSONEvent provided in the Flume SDK and use Google Gson to create the JSON string using the Gson#fromJson(Object, Type) method.
				Typetype=newTypeToken<List<JSONEvent>>(){}.getType();

			BlobHandler
				默认情况下，HTTPSource切割JSON输入为Flume enents。
				另一种情况中，BlobHandler是一种将请求中上传文件信息转化为event的处理器。
				参数说明，加！为必须属性：
					！handler	–	The FQCN of this class: org.apache.flume.sink.solr.morphline.BlobHandler
					handler.maxBlobLength	100000000	The maximum number of bytes to read and buffer for a given request	

	12.Stress Source -- 压力源
		这时用来进行压力测试的Source.
		它允许用户配置Evnet以指定的大小和空头。
		允许用户配置发送的事件总数和被输送成功的事件最大数。

		参数说明，加！的为必须属性：
			!type	–	类型，必须是“org.apache.flume.source.StressSource”
			size	500	每一个事件的有效荷载大小，以字节为单位
			maxTotalEvents	-1	要发送的最大事件数
			maxSuccessfulEvents	-1	事件最大发送成功数量
			batchSize	1	一个批中最大事件数

	13.Legacy Sources -- 传统资源
		用来使Flume1.x可以接受来自Flume0.9.4的事件。
		略。

	14.Custom source -- 自定义源
		自定义源是自己实现源接口得到的。
		自定义源的类和其依赖包必须在开始时就放置到Flume的类加载目录下。

		参数说明，加！为必须属性：
			！channels	–	 
			！type	–	类型，必须设置为自己的自定义处理类的全路径名
			selector.type	 	
			selector.*	
			interceptors	–	
			interceptors.*	 	 

	15.Scribe SOurce
		略
九、Event Deserializers 事件解析器
	Flume自带的事件解析器：
	1.LINE
		一行一事件。
		属性说明：
			deserializer.maxLineLength	2048	单事件最大字符数，如果一行超过了这个字符数，则被截断，被截掉的数据将会出现在后续的事件中。
			deserializer.outputCharset	UTF-8	事件写入通道时使用的编码.
	2.AVRO
		此解析器可以解析AVRO文件，每个AVRO中的记录成为一个事件。每个事件都标注有一个头，表示使用的模式。
		属性说明：
			deserializer.schemaType	HASH	How the schema is represented. By default, or when the value HASH is specified, the Avro schema is hashed and the hash is stored in every event in the event header “flume.avro.schema.hash”. If LITERAL is specified, the JSON-encoded schema itself is stored in every event in the event header “flume.avro.schema.literal”. Using LITERAL mode is relatively inefficient compared to HASH mode.

	3.BlobDeserializer
		该解析器读取二进制大对象。通常每个文件一个BLOB。例如PDF或JPG文件。
		注意，这种方式不太适合于非常大的对象，因为程序会将整个文件缓存在内存中。

		属性说明：
			！deserializer	–	The FQCN of this class: org.apache.flume.sink.solr.morphline.BlobDeserializer$Builder
			deserializer.maxBlobLength	100000000	支持的最大字节数
===Sink=================================================================================================
十、HDFS Sink
	此Sink将事件写入到Hadoop分布式文件系统HDFS中。
	目前它支持创建文本文件和序列化文件。
	对这两种格式都支持压缩。
	这些文件可以分卷，按照指定的时间或数据量或事件的数量为基础。
	它还通过类似时间戳或机器属性对数据进行 buckets/partitions 操作    It also buckets/partitions data by attributes like timestamp or machine where the event originated.
	HDFS的目录路径可以包含将要由HDFS替换格式的转移序列用以生成存储事件的目录/文件名。
	使用这个Sink要求haddop必须已经安装好，以便Flume可以通过hadoop提供的jar包与HDFS进行通信。
	注意，此版本hadoop必须支持sync()调用。

	以下是支持的转移序列：
		%{host}	用头名“host”的值替代
		%t	Unix时间以毫秒为单位
		%a	语言环境的短星期名称（周一、周二。。。。）
		%A	语言环境的长星期名称（星期一、星期二。。。。）
		%b	语言环境的短月名称
		%B	语言环境的长月名称
		%c	语言环境的日期时间
		%d	月中日（01）
		%e	月中日，无填充（1）
		%D	日期; 格式同 %m/%d/%y
		%H	小时 (00..23)
		%I	小时 (01..12)
		%j	年中日 (001..366)
		%k	小时( 0..23)
		%m	月 (01..12)
		%n	月，无填充 (1..12)
		%M	分 (00..59)
		%p	语言环境中的 上午/下午
		%s	从 1970-01-01 00:00:00 UTC 计算的秒
		%S	秒 (00..59)
		%y	年的后两位 (00..99)
		%Y	年 (2010)
		%z	+hhmm 数字时区 (for example, -0400)

	使用中的文件将以".tmp"为后缀。
	一旦文件被关闭，这个扩展名将被删除。
	这使得不会在目录中包含部分处理完成的文件。


	属性说明，加！的为必须属性：
		!channel	–	 
		!type	–	类型名称，必须是“HDFS”
		!hdfs.path	–	HDFS 目录路径 (eg hdfs://namenode/flume/webdata/)
		hdfs.filePrefix	FlumeData	Flume在目录下创建文件的名称前缀
		hdfs.fileSuffix	–	追加到文件的名称后缀 (eg .avro - 注: 日期时间不会自动添加)
		hdfs.inUsePrefix	–	Flume正在处理的文件所加的前缀
		hdfs.inUseSuffix	.tmp	Flume正在处理的文件所加的后缀
		hdfs.rollInterval	30	Number of seconds to wait before rolling current file (0 = never roll based on time interval)
		hdfs.rollSize	1024	File size to trigger roll, in bytes (0: never roll based on file size)
		hdfs.rollCount	10	Number of events written to file before it rolled (0 = never roll based on number of events)
		hdfs.idleTimeout	0	Timeout after which inactive files get closed (0 = disable automatic closing of idle files)
		hdfs.batchSize	100	number of events written to file before it is flushed to HDFS
		hdfs.codeC	–	Compression codec. one of following : gzip, bzip2, lzo, lzop, snappy
		hdfs.fileType	SequenceFile	File format: currently SequenceFile, DataStream or CompressedStream (1)DataStream will not compress output file and please don’t set codeC (2)CompressedStream requires set hdfs.codeC with an available codeC
		hdfs.maxOpenFiles	5000	Allow only this number of open files. If this number is exceeded, the oldest file is closed.
		hdfs.minBlockReplicas	–	Specify minimum number of replicas per HDFS block. If not specified, it comes from the default Hadoop config in the classpath.
		hdfs.writeFormat	–	Format for sequence file records. One of “Text” or “Writable” (the default).
		hdfs.callTimeout	10000	Number of milliseconds allowed for HDFS operations, such as open, write, flush, close. This number should be increased if many HDFS timeout operations are occurring.
		hdfs.threadsPoolSize	10	Number of threads per HDFS sink for HDFS IO ops (open, write, etc.)
		hdfs.rollTimerPoolSize	1	Number of threads per HDFS sink for scheduling timed file rolling
		hdfs.kerberosPrincipal	–	Kerberos user principal for accessing secure HDFS
		hdfs.kerberosKeytab	–	Kerberos keytab for accessing secure HDFS
		hdfs.proxyUser	 	 
		hdfs.round	false	时间戳是否向下取整（如果是true，会影响所有基于时间的转移序列，除了%T）
		hdfs.roundValue	1	舍值的边界值
		hdfs.roundUnit	向下舍值的单位 -  second, minute , hour
		hdfs.timeZone	Local Time	Name of the timezone that should be used for resolving the directory path, e.g. America/Los_Angeles.
		hdfs.useLocalTimeStamp	false	Use the local time (instead of the timestamp from the event header) while replacing the escape sequences.
		hdfs.closeTries	0	Number of times the sink must try renaming a file, after initiating a close attempt. If set to 1, this sink will not re-try a failed rename (due to, for example, NameNode or DataNode failure), and may leave the file in an open state with a .tmp extension. If set to 0, the sink will try to rename the file until the file is eventually renamed (there is no limit on the number of times it would try). The file may still remain open if the close call fails but the data will be intact and in this case, the file will be closed only after a Flume restart.
		hdfs.retryInterval	180	Time in seconds between consecutive attempts to close a file. Each close call costs multiple RPC round-trips to the Namenode, so setting this too low can cause a lot of load on the name node. If set to 0 or less, the sink will not attempt to close the file if the first attempt fails, and may leave the file open or with a ”.tmp” extension.
		serializer	TEXT	Other possible options include avro_event or the fully-qualified class name of an implementation of the EventSerializer.Builder interface.


		示例：
		--
		a1.channels  =  c1 
		a1.sinks  =  k1 
		a1.sinks.k1.type  =  hdfs 
		a1.sinks.k1.channel  =  c1 
		a1.sinks.k1.hdfs.path  =  /flume/events/%y-%m-%d/%H%M/%S 
		a1.sinks.k1.hdfs.filePrefix  =  events- 
		a1.sinks.k1.hdfs.round  =  true 
		a1.sinks.k1.hdfs.roundValue  =  10 
		a1.sinks.k1.hdfs.roundUnit  =  minute
		--
		以上的例子中，将时间戳四舍五入到最后的第10分钟。
		例如 时间戳上午11点54分34秒，6月12日 2012年的事件 将成为 /flume/events/2012-06-12/1150/00.

十一、Hive Sink
	这个Sink可以将含分隔符的文本或JSON数据事件直接导入Hive的表或分区。
	事件是使用Hive transactions编写的。
	当一组Event被提交到Hive中，他们立即可以通过Hive被查询出来。
	Flume要写入数据的分区即可以预先创建好，也可以在缺失时由Flume来创建。
	收到的数据字段将映射到Hive表的列上。
	此功能是一个预览功能，不推荐在生产环境下使用。

	属性说明，加！的为必须字段：
		！channel	–	 
		！type	–	类型，必须设置为“hive”
		！hive.metastore	–	Hive metastore URI (eg thrift://a.b.com:9083 )
		！hive.database	–	hive库名称
		！hive.table	–	hive表名称
		hive.partition	–	逗号分开的分区值确定写入分区的列表。可以包含转义序列。
							Comma separate list of partition values identifying the partition to write to. May contain escape sequences. E.g: If the table is partitioned by (continent: string, country :string, time : string) then ‘Asia,India,2014-02-26-01-21’ will indicate continent=Asia,country=India,time=2014-02-26-01-21
		hive.txnsPerBatchAsk	100	Hive grants a batch of transactions instead of single transactions to streaming clients like Flume. This setting configures the number of desired transactions per Transaction Batch. Data from all transactions in a single batch end up in a single file. Flume will write a maximum of batchSize events in each transaction in the batch. This setting in conjunction with batchSize provides control over the size of each file. Note that eventually Hive will transparently compact these files into larger files.
		heartBeatInterval	240	(In seconds) Interval between consecutive heartbeats sent to Hive to keep unused transactions from expiring. Set this value to 0 to disable heartbeats.
		autoCreatePartitions	true	Flume will automatically create the necessary Hive partitions to stream to
		batchSize	15000	Max number of events written to Hive in a single Hive transaction
		maxOpenConnections	500	Allow only this number of open connections. If this number is exceeded, the least recently used connection is closed.
		callTimeout	10000	(In milliseconds) Timeout for Hive & HDFS I/O operations, such as openTxn, write, commit, abort.
		serializer	 	Serializer is responsible for parsing out field from the event and mapping them to columns in the hive table. Choice of serializer depends upon the format of the data in the event. Supported serializers: DELIMITED and JSON
		roundUnit	minute	The unit of the round down value - second, minute or hour.
		roundValue	1	Rounded down to the highest multiple of this (in the unit configured using hive.roundUnit), less than current time
		timeZone	Local Time	Name of the timezone that should be used for resolving the escape sequences in partition, e.g. America/Los_Angeles.
		useLocalTimeStamp	false	Use the local time (instead of the timestamp from the event header) while replacing the escape sequences.

		。。。
		略
		。。。
		
十二、Logger Sink
	记录INFO级别的日志，通常用于调试。
	属性说明：
		channel	–	 
		type	–	The component type name, needs to be logger
		maxBytesToLog	16	Maximum number of bytes of the Event body to log

十三、Avro Sink
	This sink forms one half of Flume's tiered collection support
	发送到此Sink的事件转换为Avro事件发送到指定的Host/port对上。
	The events are taken from the configured Channel in batches of the configured batch size.

	属性说明：
		channel	–	 
			!type	–	The component type name, needs to be avro.
			!hostname	–	The hostname or IP address to bind to.
			!port	–	The port # to listen on.
			batch-size	100	number of event to batch together for send.
			connect-timeout	20000	Amount of time (ms) to allow for the first (handshake) request.
			request-timeout	20000	Amount of time (ms) to allow for requests after the first.
			reset-connection-interval	none	Amount of time (s) before the connection to the next hop is reset. This will force the Avro Sink to reconnect to the next hop. This will allow the sink to connect to hosts behind a hardware load-balancer when news hosts are added without having to restart the agent.
			compression-type	none	This can be “none” or “deflate”. The compression-type must match the compression-type of matching AvroSource
			compression-level	6	The level of compression to compress event. 0 = no compression and 1-9 is compression. The higher the number the more compression
			ssl	false	Set to true to enable SSL for this AvroSink. When configuring SSL, you can optionally set a “truststore”, “truststore-password”, “truststore-type”, and specify whether to “trust-all-certs”.
			trust-all-certs	false	If this is set to true, SSL server certificates for remote servers (Avro Sources) will not be checked. This should NOT be used in production because it makes it easier for an attacker to execute a man-in-the-middle attack and “listen in” on the encrypted connection.
			truststore	–	The path to a custom Java truststore file. Flume uses the certificate authority information in this file to determine whether the remote Avro Source’s SSL authentication credentials should be trusted. If not specified, the default Java JSSE certificate authority files (typically “jssecacerts” or “cacerts” in the Oracle JRE) will be used.
			truststore-password	–	The password for the specified truststore.
			truststore-type	JKS	The type of the Java truststore. This can be “JKS” or other supported Java truststore type.
			exclude-protocols	SSLv3	Space-separated list of SSL/TLS protocols to exclude. SSLv3 will always be excluded in addition to the protocols specified.
			maxIoWorkers	2 * the number of available processors in the machine	The maximum number of I/O w
十四、Thrift Sink
	略

十五、IRC Sink
	略
	
十六、File Roll Sink
	在本地文件系统中存储事件。
	属性说明：
		!channel	–	 
		!type	–	类型，必须是"file_roll"
		!sink.directory	–	文件被存储的目录
		sink.rollInterval	30	滚动文件每隔30秒（应该是每隔30秒钟单独切割数据到一个文件的意思）。如果设置为0，则禁止滚动，从而导致所有数据被写入到一个文件中。
		sink.serializer	TEXT	Other possible options include avro_event or the FQCN of an implementation of EventSerializer.Builder interface.
		batchSize	100	 


十七、Null Sink
	丢弃它接受到的来自Channle的所有事件。

	属性说明:
		channel	–	 
		type	–	类型，必须是"null"
		batchSize	100	 
	
十八、HBaseSinks
	略
十九、MorphlineSolrSink
	略
二十、ElasticSearchSink
	略
二十一、Kite Dataset
	略
二十二、Kafka Sink
	略
二十三、Custom Sink
	自定义接收器，是自己实现的接收器接口Sink来实现的。
	自定义接收器的类及其依赖类须在Flume启动前放置到Flume类加载目录下。

	属性说明：
		type	–	类型，需要指定为自己实现的Sink类的全路径名


===Channle=================================================================================================
二十四、Memory Channel 内存通道
	事件将被存储在内存中的具有指定大小的队列中。
	非常适合那些需要高吞吐量但是失败是会丢失数据的场景下。

	属性说明：
		type	–	类型，必须是“memory”
		capacity	100	事件存储在信道中的最大数量
		transactionCapacity	100	每个事务中的最大事件数
		keep-alive	3	添加或删除操作的超时时间
		byteCapacityBufferPercentage	20	Defines the percent of buffer between byteCapacity and the estimated total size of all events in the channel, to account for data in headers. See below.
		byteCapacity	see description	Maximum total bytes of memory allowed as a sum of all events in this channel. The implementation only counts the Event body, which is the reason for providing the byteCapacityBufferPercentage configuration parameter as well. Defaults to a computed value equal to 80% of the maximum memory available to the JVM (i.e. 80% of the -Xmx value passed on the command line). Note that if you have multiple memory channels on a single JVM, and they happen to hold the same physical events (i.e. if you are using a replicating channel selector from a single source) then those event sizes may be double-counted for channel byteCapacity purposes. Setting this value to 0 will cause this value to fall back to a hard internal limit of about 200 GB.
二十五、JDBC Channel
	事件被持久存储在可靠的数据库中。目前支持嵌入式的Derby数据库。如果可恢复性非常的重要可以使用这种方式。

	属性说明：
		type	–	类型，必须是“jdbc”
		db.type	DERBY	数据库类型，必须是“DERBY”
		driver.class	org.apache.derby.jdbc.EmbeddedDriver	Class for vendor’s JDBC driver
		driver.url	(constructed from other properties)	JDBC connection URL
		db.username	“sa”	User id for db connection
		db.password	–	password for db connection
		connection.properties.file	–	JDBC Connection property file path
		create.schema	true	If true, then creates db schema if not there
		create.index	true	Create indexes to speed up lookups
		create.foreignkey	true	 
		transaction.isolation	“READ_COMMITTED”	Isolation level for db session READ_UNCOMMITTED, READ_COMMITTED, SERIALIZABLE, REPEATABLE_READ
		maximum.connections	10	Max connections allowed to db
		maximum.capacity	0 (unlimited)	Max number of events in the channel
		sysprop.*	 	DB Vendor specific properties
		sysprop.user.home	 	Home path to store embedded Derby database

二十六、Kafka channel
	略

二十七、File Channel
	属性说明:
		!type	–	类型，必须是“file”
		checkpointDir	~/.flume/file-channel/checkpoint	检查点文件存放的位置
		useDualCheckpoints	false	Backup the checkpoint. If this is set to true, backupCheckpointDir must be set
		backupCheckpointDir	–	The directory where the checkpoint is backed up to. This directory must not be the same as the data directories or the checkpoint directory
		dataDirs	~/.flume/file-channel/data	逗号分隔的目录列表，用以存放日志文件。使用单独的磁盘上的多个目录可以提高文件通道效率。
		transactionCapacity	10000	The maximum size of transaction supported by the channel
		checkpointInterval	30000	Amount of time (in millis) between checkpoints
		maxFileSize	2146435071	一个日志文件的最大尺寸
		minimumRequiredSpace	524288000	Minimum Required free space (in bytes). To avoid data corruption, File Channel stops accepting take/put requests when free space drops below this value
		capacity	1000000	Maximum capacity of the channel
		keep-alive	3	Amount of time (in sec) to wait for a put operation
		use-log-replay-v1	false	Expert: Use old replay logic
		use-fast-replay	false	Expert: Replay without using queue
		checkpointOnClose	true	Controls if a checkpoint is created when the channel is closed. Creating a checkpoint on close speeds up subsequent startup of the file channel by avoiding replay.
		encryption.activeKey	–	Key name used to encrypt new data
		encryption.cipherProvider	–	Cipher provider type, supported types: AESCTRNOPADDING
		encryption.keyProvider	–	Key provider type, supported types: JCEKSFILE
		encryption.keyProvider.keyStoreFile	–	Path to the keystore file
		encrpytion.keyProvider.keyStorePasswordFile	–	Path to the keystore password file
		encryption.keyProvider.keys	–	List of all keys (e.g. history of the activeKey setting)
		encyption.keyProvider.keys.*.passwordFile	–	Path to the optional key password file

		。。
		略
		。。

二十八、Spillable Memory Channel -- 内存溢出通道
	事件被存储在内存队列和磁盘中。
	内存队列作为主存储，而磁盘作为溢出内容的存储。
	内存存储通过embedded File channel来进行管理。
	当内存队列已满时，后续的事件将被存储在文件通道中。 
	这个通道适用于正常操作期间适用内存通道已期实现高效吞吐，而在高峰期间适用文件通道实现高耐受性。通过降低吞吐效率提高系统可耐受性。
	如果Agent崩溃，则只有存储在文件系统中的事件可以被恢复。
	此通道处于试验阶段，不建议在生产环境中使用。 

	属性说明：
	!type	–	类型，必须是"SPILLABLEMEMORY"
	memoryCapacity	10000	内存中存储事件的最大值，如果想要禁用内存缓冲区将此值设置为0。
	overflowCapacity	100000000	可以存储在磁盘中的事件数量最大值。设置为0可以禁用磁盘存储。 
	overflowTimeout	3	The number of seconds to wait before enabling disk overflow when memory fills up.
	byteCapacityBufferPercentage	20	Defines the percent of buffer between byteCapacity and the estimated total size of all events in the channel, to account for data in headers. See below.
	byteCapacity	see description	Maximum bytes of memory allowed as a sum of all events in the memory queue. The implementation only counts the Event body, which is the reason for providing the byteCapacityBufferPercentage configuration parameter as well. Defaults to a computed value equal to 80% of the maximum memory available to the JVM (i.e. 80% of the -Xmx value passed on the command line). Note that if you have multiple memory channels on a single JVM, and they happen to hold the same physical events (i.e. if you are using a replicating channel selector from a single source) then those event sizes may be double-counted for channel byteCapacity purposes. Setting this value to 0 will cause this value to fall back to a hard internal limit of about 200 GB.
	avgEventSize	500	Estimated average size of events, in bytes, going into the channel
	<file channel properties>	see file channel	Any file channel property with the exception of ‘keep-alive’ and ‘capacity’ can be used. The keep-alive of file channel is managed by Spillable Memory Channel. Use ‘overflowCapacity’ to set the File channel’s capacity.

二十九、Pseudo Transaction Channel
	仅用于测试，不建议在生产环境下使用。
	略。

三十、自定义渠道
	自定义渠道需要自己实现Channel接口。
	自定义Channle类及其依赖类必须在Flume启动前放置到类加载的目录下。

	参数说明：
		type - 自己实现的Channle类的全路径名称


===Selector=================================================================================================
	选择器有两种工作方式，复制、多路复用。如果不明确指定，默认为"复制"。
三十一、复制通道选择器
	属性说明：
		selector.type	类型名称，必须是 replicating
		selector.optional	–	标志通道为可选

三十二、多路复用通道选择器
	属性说明：
	selector.type	类型，必须是"multiplexing"
	selector.header	flume.selector.header	 
	selector.default	–	 
	selector.mapping.*	–
		
三十三、自定义通道选择器
	自己来实现ChannelSelector接口。
	自定义选择器必须在Flume启动前将类及其依赖类放置到Flume的类加载目录下。 

	属性说明:
		selector.type - 类型，指定为自己实现的Selector类的全路径名。


===Flume Sink Processors=================================================================================================
	Sink Group允许用户将多个Sink组合成一个实体。
	Flume Sink Processor 可以通过切换组内Sink用来实现负载均衡的效果，或在一个Sink故障时切换到另一个Sink。

	sinks	–	用空格分隔的Sink集合
	processor.type	default	类型名称，必须是 default、failover 或 load_balance

三十四、Default Sink Processor

	Default Sink Processor 只接受一个 Sink。
	不要求用户为单一Sink创建processor

三十五、Failover Sink Processor
	Failover Sink Processor 维护一个sink们的优先表。确保只要一个是可用的就事件就可以被处理。
	失败处理原理是，为失效的sink指定一个冷却时间，在冷却时间到达后再重新使用。
	sink们可以被配置一个优先级，数字越大优先级越高。
	如果sink发送事件失败，则下一个最高优先级的sink将会尝试接着发送事件。
	如果没有指定优先级，则优先级顺序取决于sink们的配置顺序，先配置的默认优先级高于后配置的。
	在配置的过程中，设置一个group processor ，并且为每个sink都指定一个优先级。
	优先级必须是唯一的。
	另外可以设置maxpenalty属性指定限定失败时间。

	sinks	–	Space-separated list of sinks that are participating in the group
	processor.type	default	The component type name, needs to be failover
	processor.priority.<sinkName>	–	Priority value. <sinkName> must be one of the sink instances associated with the current sink group A higher priority value Sink gets activated earlier. A larger absolute value indicates higher priority
	processor.maxpenalty	30000	The maximum backoff period for the failed Sink (in millis)

	Example for agent named a1:
	------
	a1.sinkgroups = g1
	a1.sinkgroups.g1.sinks = k1 k2
	a1.sinkgroups.g1.processor.type = failover
	a1.sinkgroups.g1.processor.priority.k1 = 5
	a1.sinkgroups.g1.processor.priority.k2 = 10
	a1.sinkgroups.g1.processor.maxpenalty = 10000
	------
	
三十六、Load balancing Sink Processor
	Load balancing Sink processor 提供了在多个sink之间实现负载均衡的能力。
	它维护了一个活动sink的索引列表。
	它支持轮询 或 随机方式的负载均衡，默认值是轮询方式，可以通过配置指定。
	也可以通过实现AbstractSinkSelector接口实现自定义的选择机制。

	!processor.sinks	–	Space-separated list of sinks that are participating in the group
	!processor.type	default	The component type name, needs to be load_balance
	processor.backoff	false	Should failed sinks be backed off exponentially.
	processor.selector	round_robin	Selection mechanism. Must be either round_robin, random or FQCN of custom class that inherits from AbstractSinkSelector
	processor.selector.maxTimeOut	30000	Used by backoff selectors to limit exponential backoff (in milliseconds)

	------
	a1.sinkgroups = g1
	a1.sinkgroups.g1.sinks = k1 k2
	a1.sinkgroups.g1.processor.type = load_balance
	a1.sinkgroups.g1.processor.backoff = true
	a1.sinkgroups.g1.processor.selector = random
	------

三十七、自定义接受处理器
	目前不支持


===Event Serializers=================================================================================================
File Roll Sink 和 hdfs Sink都支持 EventSerializer
三十八、Body Text Serializer
	这个拦截器将事件体不经改造或改写直接写入到输出流中。事件头将被忽略。
	配置选项如下：
		appendNewline	true	在写出时是否在每个事件后写入一个换行符。默认true表示不增加换行符（历史遗留原因）


三十九、Avro Event Serializer
	这个拦截器序列化Flume Event到一个Avro Container File。The schema used is the same schema used for Flume events in the Avro RPC mechanism.这个类继承自AbstractAvroEventSerializer类。
	配置如下：
		syncIntervalBytes	2048000	Avro sync interval, in approximate bytes.
		compressionCodec	null	Avro compression codec. For supported codecs, see Avro’s CodecFactory docs.


===Flume Interceptors=================================================================================================
	Flume有能力在运行阶段修改/删除Event，这时通过拦截器（Interceptors）来实现的。
	拦截器需要实现org.apache.flume.interceptor.Interceptor接口。
	拦截器可以修改或删除事件基于开发者在选择器中选择的任何条件。
	拦截器采用了责任链模式，多个拦截器可以按指定顺序拦截。
	一个拦截器返回的事件列表被传递给链中的下一个拦截器。
	如果一个拦截器需要删除事件，它只需要在返回的事件集中不包含要删除的事件即可。
	如果要删除所有事件，只需返回一个空列表。
	
四十、Timestamp Interceptor
	这个拦截器在事件头中插入以毫秒为单位的当前处理时间。
	头的名字为timestamp，值为当前处理的时间戳。
	如果在之前已经有这个时间戳，则保留原有的时间戳。

	参数说明：
		!type	–	类型名称，必须是timestamp或自定义类的全路径名
		preserveExisting	false	如果时间戳已经存在是否保留

四十一、Host Interceptor
	这个拦截器插入当前处理Agent的主机名或ip
	头的名字为host或配置的名称
	值是主机名或ip地址，基于配置。

	参数说明：
		!type	–	类型名称，必须是host
		preserveExisting	false	如果主机名已经存在是否保留
		useIP	true	如果配置为true则用IP，配置为false则用主机名
		hostHeader	host	加入头时使用的名称

四十二、Static Interceptor
	此拦截器允许用户增加静态头信息使用静态的值到所有事件。
	目前的实现中不允许一次指定多个头。
	如果需要增加多个静态头可以指定多个Static interceptors
	属性说明:
		!type	–	类型，必须是static
		preserveExisting	true	如果配置头已经存在是否应该保留
		key	key	要增加的透明
		value	value	要增加的头值

四十三、UUID Interceptor
	这个拦截器在所有事件头中增加一个全局一致性标志。
	其实就是UUID。

	属性说明：
		!type	–	类型名称，必须是org.apache.flume.sink.solr.morphline.UUIDInterceptor$Builder
		headerName	id	头名称
		preserveExisting	true	如果头已经存在，是否保留
		prefix	“”	在UUID前拼接的字符串前缀
		

四十四、Morphline  Interceptor


四十五、Search and Replace Interceptor
	这个拦截器提供了简单的基于字符串的正则搜索和替换功能。

	属性说明：
		type	–	类型名称，必须是"search_replace"
		searchPattern	–	要搜索和替换的正则表达式
		replaceString	–	要替换为的字符串
		charset	UTF-8	字符集编码，默认utf-8

四十六、Regex Filtering Interceptor
	此拦截器通过解析事件体去匹配给定正则表达式来筛选事件。
	所提供的正则表达式即可以用来包含或刨除事件。

	属性说明：
	!type	–	类型，必须设定为regex_filter
	regex	”.*” 所要匹配的正则表达式
	excludeEvents	false	如果是true则刨除匹配的事件，false则包含匹配的事件。

四十七、Regex Extractor Interceptor
	使用指定正则表达式匹配事件，并将匹配到的组作为头加入到事件中。
	它也支持插件化的序列化器用来格式化匹配到的组在加入他们作为头之前。

	属性说明：
		!type	–	类型，必须是regex_extractor
		!regex	–	要匹配的正则表达式
		!serializers	–	Space-separated list of serializers for mapping matches to header names and serializing their values. (See example below) Flume provides built-in support for the following serializers: org.apache.flume.interceptor.RegexExtractorInterceptorPassThroughSerializer org.apache.flume.interceptor.RegexExtractorInterceptorMillisSerializer
		serializers.<s1>.type	default	Must be default (org.apache.flume.interceptor.RegexExtractorInterceptorPassThroughSerializer), org.apache.flume.interceptor.RegexExtractorInterceptorMillisSerializer, or the FQCN of a custom class that implements org.apache.flume.interceptor.RegexExtractorInterceptorSerializer
		serializers.<s1>.name	–	 
		serializers.*	–	Serializer-specific properties

	----
	If the Flume event body contained 1:2:3.4foobar5 and the following configuration was used

	a1.sources.r1.interceptors.i1.regex = (\\d):(\\d):(\\d)
	a1.sources.r1.interceptors.i1.serializers = s1 s2 s3
	a1.sources.r1.interceptors.i1.serializers.s1.name = one
	a1.sources.r1.interceptors.i1.serializers.s2.name = two
	a1.sources.r1.interceptors.i1.serializers.s3.name = three

	The extracted event will contain the same body but the following headers will have been added one=>1, two=>2, three=>3
	----
	
四十八、Log4jAppender
	flume为log4j框架提供了专用的Appender，可以将日志信息通过avro形式发送到flume中。
	首先需要将支持包放入类加载路径下。
	在配置log4j配置文件，属性如下：

	!Hostname	–	The hostname on which a remote Flume agent is running with an avro source.
	!Port	–	The port at which the remote Flume agent’s avro source is listening.
	UnsafeMode	false	If true, the appender will not throw exceptions on failure to send the events.
	AvroReflectionEnabled	false	Use Avro Reflection to serialize Log4j events. (Do not use when users log strings)
	AvroSchemaUrl	–	A URL from which the Avro schema can be retrieved.

	---
	#...
	log4j.appender.flume = org.apache.flume.clients.log4jappender.Log4jAppender
	log4j.appender.flume.Hostname = example.com
	log4j.appender.flume.Port = 41414
	log4j.appender.flume.UnsafeMode = true

	# configure a class's logger to output to the flume appender
	log4j.logger.org.example.MyClass = DEBUG,flume
	#...
	---

四十九、Load Balancing Log4j Appender
	可以实现负载均衡的的Appender，可以将日志信息通过avro形式发送到flume中，并根据配置均衡负载。
	首先需要将支持包放入类加载路径下。
	在配置log4j配置文件，属性如下：

	!Hosts	–	A space-separated list of host:port at which Flume (through an AvroSource) is listening for events
	Selector	ROUND_ROBIN	Selection mechanism. Must be either ROUND_ROBIN, RANDOM or custom FQDN to class that inherits from LoadBalancingSelector.
	MaxBackoff	–	A long value representing the maximum amount of time in milliseconds the Load balancing client will backoff from a node that has failed to consume an event. Defaults to no backoff
	UnsafeMode	false	If true, the appender will not throw exceptions on failure to send the events.
	AvroReflectionEnabled	false	Use Avro Reflection to serialize Log4j events.
	AvroSchemaUrl	–	A URL from which the Avro schema can be retrieved.


	---
	#...
	log4j.appender.out2 = org.apache.flume.clients.log4jappender.LoadBalancingLog4jAppender
	log4j.appender.out2.Hosts = localhost:25430 localhost:25431

	# configure a class's logger to output to the flume appender
	log4j.logger.org.example.MyClass = DEBUG,flume
	---


























